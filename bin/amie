#!/usr/bin/env python

import time

from random import randint

def randsleep(max=300):
    time.sleep(max/2 + randint(max/2, max))

def download_pages(start=1, end=10, perpage=250, notdled=True):
    notdled = notdled == True and 1 or 0
    while start <= end:
        yield "/downloads?&perPage=%d&page=%d&notDled=%d" % (
            perpage, start, notdled)
    start += 1

downloadable = lambda link: "download/album" in link.absolute_url

if __name__ == "__main__":
    import logging
    import os
    import sys
    import urllib2

    sys.path.extend([
        '/scratch/wcmaier/lib/python2.5/site-packages',
        '/scratch/wcmaier/lib/python2.5'
    ])

    import twill

    from twill import get_browser
    from twill.commands import formvalue, submit

    USERNAME = "lt_kije"
    PASSWORD = "lesheros"
    USER_AGENT = "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; " \
        "rv:1.9.1b2) Gecko/20081127 Firefox/3.1b1"
    LOG_FMT = "%(asctime)s %(message)s"
    LOG_LEVEL = logging.DEBUG

    # Configure logging.
    logging.basicConfig(level=LOG_LEVEL, format=LOG_FMT)
    log = logging.getLogger('.')

    downloads = set()

    null = open(os.devnull, 'w')
    twill.set_output(null)

    browser = get_browser()
    browser.links = browser._browser.links
    browser.set_agent_string(USER_AGENT)

    signin = "https://amiestreet.com/auth/login?ssl=1"
    log.debug("Signing in at %s", signin)
    browser.go(signin)

    # Login.
    log.debug("Logging in as user '%s'", USERNAME)
    formvalue('1', "login-username", USERNAME)
    formvalue('1', "login-password", PASSWORD)
    submit()

    # Look for album download links.
    last = False
    for page in download_pages():
        if page == last:
            break
        browser.go(page)
        albums = set([link.absolute_url for link in browser.links() if downloadable(link)])
        if albums:
            log.debug("Discovered %d download links on %s",
                len(albums), page)
            downloads.update(albums)
        else:
            break
        last = page

    # Prepare for downloading.
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(browser.cj))
    urllib2.install_opener(opener)
    headers = {"User-Agent": USER_AGENT}

    # Fetch files.
    for url in downloads:
        target = os.path.basename(url)
        if os.path.exists(target):
            log.debug("Skipping %s; already exists", url)
            continue

        log.debug("Fetching %s", url)
        try:
            req = urllib2.Request(url, headers=headers)
            handle = urllib2.urlopen(req)
        except Exception, e:
            log.warn("Failed to fetch %s: %s", url, e)
            continue

        try:
            f = open(target, 'w')
            f.write(handle.read())
        except:
            log.warn("Failed to write %s (%s)", target, url)

        f.close()
        handle.close()

        # Be polite/devious and sleep a little bit before moving on
        # to the next file.
        randsleep(300)
