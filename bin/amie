#!/usr/bin/env python

import logging
import os
import sys
import time
import urllib2

from random import randint

import twill

from twill import get_browser
from twill.commands import formvalue, submit

USERNAME = "lt_kije"
PASSWORD = "lesheros"
USER_AGENT = "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; " \
    "rv:1.9.1b2) Gecko/20081127 Firefox/3.1b1"

def randsleep(max=300):
    time.sleep(max/2 + randint(max/2, max))

def download_pages(start=1, end=10, perpage=250, notdled=True):
    notdled = notdled == True and 1 or 0
    while start <= end:
        yield "/downloads?&perPage=%d&page=%d&notDled=%d" % (
            perpage, start, notdled)
    start += 1

downloadable = lambda link: "download/album" in link.absolute_url

def amie(app, url=None):
    """[-hqsv] [command] [url]"""
    downloads = set()

    null = open(os.devnull, 'w')
    twill.set_output(null)

    browser = get_browser()
    browser.links = browser._browser.links
    browser.set_agent_string(USER_AGENT)

    signin = "https://amiestreet.com/auth/login?ssl=1"
    app.log.debug("Signing in at %s", signin)
    browser.go(signin)

    # Login.
    app.log.debug("Logging in as user '%s'", USERNAME)
    formvalue('1', "login-username", USERNAME)
    formvalue('1', "login-password", PASSWORD)
    submit()

    # Look for album download links.
    last = False
    for page in download_pages():
        if page == last:
            break
        browser.go(page)
        albums = set([link.absolute_url for link in browser.links() if downloadable(link)])
        if albums:
            app.log.debug("Discovered %d download links on %s",
                len(albums), page)
            downloads.update(albums)
        else:
            break
        last = page

    # Prepare for downloading.
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(browser.cj))
    urllib2.install_opener(opener)
    headers = {"User-Agent": USER_AGENT}

    # Fetch files.
    for url in downloads:
        target = os.path.basename(url)
        if os.path.exists(target):
            app.log.debug("Skipping %s; already exists", url)
            continue

        app.log.debug("Fetching %s", url)
        try:
            req = urllib2.Request(url, headers=headers)
            handle = urllib2.urlopen(req)
        except Exception, e:
            app.log.warn("Failed to fetch %s: %s", url, e)
            continue

        try:
            f = open(target, 'w')
            f.write(handle.read())
        except:
            app.log.warn("Failed to write %s (%s)", target, url)

        f.close()
        handle.close()

        # Be polite/devious and sleep a little bit before moving on
        # to the next file.
        randsleep(300)

    return 0

if __name__ == "__main__":
    from cli.app import App

    app = App(amie)
    app.run()
